{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Network Architecture.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/divya-r-kamat/DeepVision/blob/main/Neural%20Network%20Architecture/MNIST_Neural_Network_Architecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLvWPrJlwoFt"
      },
      "source": [
        "## Import functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m2JWFliFfKT"
      },
      "source": [
        "from __future__ import print_function #will allow to use print as a function, they must be at the top of the file\n",
        "\n",
        "import torch  # import torch \n",
        "\n",
        "import torch.nn as nn # torch neural network modules and classes, that help in creating and training of the neural network\n",
        "\n",
        "#The nn.functional package contains many useful loss functions and several other utilities.\n",
        "import torch.nn.functional as F \n",
        "\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNlScCniArwT"
      },
      "source": [
        "## Model\n",
        "We will use a convolutional neural network, using the nn.Conv2d class from PyTorch. The 2D convolution is a fairly simple operation: start with a kernel, which is simply a small matrix of weights. This kernel “slides” over the 2D input data, performing an elementwise multiplication with the part of the input it is currently on, and then summing up the results into a single output pixel.\n",
        "\n",
        "The activation function we'll use here is called a Rectified Linear Unit or ReLU, and it has a really simple formula: relu(x) = max(0,x) i.e. if an element is negative, we replace it by 0, otherwise we leave it unchanged.\n",
        "\n",
        "To define the model, we extend the nn.Module class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_Cx9q2QFgM7"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)              # input - 28 x 28 x 1   | Output - 28 x 28 x 32  | Receptive Field - 3 x 3\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)             # input - 28 x 28 x 32  | Output - 28 x 28 x 64  | Receptive Field - 5 x 5\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)                          # input - 28 x 28 x 64  | Output - 14 x 14 x 64  | Receptive Field - 10 x 10  (for now we are considering that with max pooling, receptive field doubles - but this is not entirely correct)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)            # input - 14 x 14 x 64  | Output - 14 x 14 x 128 | Receptive Field - 12 x 12\n",
        "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)           # input - 14 x 14 x 128 | Output - 14 x 14 x 256 | Receptive Field - 14 x 14\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)                          # input - 14 x 14 x 256 | Output - 7 x 7 x 256   | Receptive Field - 28 x 28\n",
        "        self.conv5 = nn.Conv2d(256, 512, 3)                      # input - 7 x 7 x 256   | Output - 5 x 5 x 512   | Receptive Field - 30 x 30\n",
        "        self.conv6 = nn.Conv2d(512, 1024, 3)                     # input - 5 x 5 x 512   | Output - 3 x 3 x 1024  | Receptive Field - 32 x 32\n",
        "        self.conv7 = nn.Conv2d(1024, 10, 3)                      # input - 3 x 3 x 1024  | Output - 1 x 1 x 10    | Receptive Field - 34 x 34\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        #The first convolutional layer self.conv1 has a convolutional operation on input tensor x, followed by a relu activation operation \n",
        "        #whcih is then passed to second convolution operation self.conv2 followed by a relu whose output is then passed to a max pooling \n",
        "        x = self.pool1(F.relu(self.conv2(F.relu(self.conv1(x))))) \n",
        "\n",
        "        #The output of the max pool operation is passed to another two convolution and relu activation operation followed by max pooling. \n",
        "        #The relu() and the max_pool2d() calls are just pure operations. Neither of these have weights  \n",
        "        x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))\n",
        "\n",
        "        #The output of the max pool operation from 4th convolution is passed to another two sets of convolution and relu activation operation\n",
        "        x = F.relu(self.conv6(F.relu(self.conv5(x))))\n",
        "\n",
        "        # The output from 6th convolution is then passed to a final convolution followed by a relu operation\n",
        "        x = F.relu(self.conv7(x))\n",
        "\n",
        "        #The flatten operation puts all of the tensor's elements into a single dimension.\n",
        "        x = x.view(-1, 10)\n",
        "\n",
        "        # Inside the network we usually use relu() as our non-linear activation function, but for the output layer, whenever we have a single category that we are trying to predict, we use softmax(). \n",
        "        #The softmax function returns a positive probability for each of the prediction classes, and the values sum to 1.\n",
        "\n",
        "        return F.log_softmax(x)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwLEMI3mDScD"
      },
      "source": [
        "## Using a GPU\n",
        "As the sizes of our models and datasets increase, we need to use GPUs to train our models within a reasonable amount of time. GPUs contain hundreds of cores that are optimized for performing expensive matrix operations on floating point numbers in a short time, which makes them ideal for training deep neural networks with many layers.\n",
        "\n",
        "Reference - [Machine Learning on GPU](https://hsf-training.github.io/hsf-training-ml-gpu-webpage/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiv1AG1SB6Sh",
        "outputId": "7a74febc-0d05-4952-f87c-3003221b22e6"
      },
      "source": [
        "#This command will return a boolean (True/False) based on the GPU availability.\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "\n",
        "#Find out the specifications of the GPU(s)\n",
        "# There are a wide variety of GPUs available these days, so it’s often useful to check the specifications of the GPU(s) that are available to you. For example, the following lines of code will tell you \n",
        "      #(i) which version of CUDA the GPU(s) support, \n",
        "      #(ii) how many GPUs there are available, \n",
        "      #(iii) for a specific GPU (here 0) what kind of GPU it is, and \n",
        "      #(iv) how much memory it has available in total.\n",
        "\n",
        "if use_cuda:\n",
        "    print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
        "    print('__Number CUDA Devices:', torch.cuda.device_count())\n",
        "    print('__CUDA Device Name:',torch.cuda.get_device_name(0))\n",
        "    print('__CUDA Device Total Memory [GB]:',torch.cuda.get_device_properties(0).total_memory/1e9)\n",
        "\n",
        "\n",
        "#use the use_cuda flag to specify which device you want to use\n",
        "# This will set the device to the GPU if one is available and to the CPU if there isn’t a GPU available. This means that you don’t need to hard code changes into your code to use one or the other.\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__CUDNN VERSION: 7603\n",
            "__Number CUDA Devices: 1\n",
            "__CUDA Device Name: Tesla T4\n",
            "__CUDA Device Total Memory [GB]: 15.843721216\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5oE_U64CkYC",
        "outputId": "dea5fe19-c09a-4f45-8e7a-ed79138cf341"
      },
      "source": [
        "# Install and import torchsummary, this helps in visualization of the model which is very helpful while debugging the network\n",
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "\n",
        "#move the model to the choosen device\n",
        "model = Net().to(device)\n",
        "\n",
        "# prints the model summary\n",
        "summary(model, input_size=(1, 28, 28))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.7/dist-packages (1.5.1)\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 28, 28]             320\n",
            "            Conv2d-2           [-1, 64, 28, 28]          18,496\n",
            "         MaxPool2d-3           [-1, 64, 14, 14]               0\n",
            "            Conv2d-4          [-1, 128, 14, 14]          73,856\n",
            "            Conv2d-5          [-1, 256, 14, 14]         295,168\n",
            "         MaxPool2d-6            [-1, 256, 7, 7]               0\n",
            "            Conv2d-7            [-1, 512, 5, 5]       1,180,160\n",
            "            Conv2d-8           [-1, 1024, 3, 3]       4,719,616\n",
            "            Conv2d-9             [-1, 10, 1, 1]          92,170\n",
            "================================================================\n",
            "Total params: 6,379,786\n",
            "Trainable params: 6,379,786\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 1.51\n",
            "Params size (MB): 24.34\n",
            "Estimated Total Size (MB): 25.85\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDkPzN6lBkbQ"
      },
      "source": [
        "## DataLoader\n",
        "\n",
        "We download the data and create a PyTorch dataset using the MNIST class from torchvision.datasets and create PyTorch data loaders for training and testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqTWLaM5GHgH"
      },
      "source": [
        "#Set the torch seed for reproducibility\n",
        "torch.manual_seed(1)\n",
        "\n",
        "#set the batch_size\n",
        "batch_size = 128\n",
        "\n",
        "# Host to GPU copies are much faster when they originate from pinned (page-locked) memory. \n",
        "# For data loading, passing pin_memory=True to a DataLoader will automatically put the fetched data Tensors in pinned memory, and thus enables faster data transfer to CUDA-enabled GPUs.\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "\n",
        "#load train data and normalize the pixel values with mean and std computed on the MNIST training set\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "\n",
        "#load test data and normalize the pixel values with mean and std computed on the MNIST training set\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LQVeYO0BXJg"
      },
      "source": [
        "## Model Training\n",
        "\n",
        "Steps for training is defined below:\n",
        "\n",
        "- forward pass : compute predicted outputs by passing inputs to the model\n",
        "- calculate the loss\n",
        "- Backward pass\n",
        "- perform single optimization step (parameter update)\n",
        "- Clear the gradients for all optimized variables\n",
        "- update average training loss\n",
        "\n",
        "Test the trained Network\n",
        "\n",
        "Finally, we test our best model on previously unseen test data and evaluate its performance. Testing on unseen data is good way to check our model perforamance to see if our model generalizes well. It may be useful to be granular in this analysis and take a look at how this model performs on each class as well as looking at its overall loss and accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fDefDhaFlwH"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "#\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}')\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "\n",
        "    #sets all the layers in the model to evaluation mode. For eg. layers like dropout layers which turn \"off\" nodes during training with some probability, but allow every node to be on during evaluation.\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMWbLWO6FuHb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7c66ec8-78b6-4392-dcbd-658818033ba2"
      },
      "source": [
        "\n",
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "for epoch in range(1, 2):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/469 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "loss=1.976784110069275 batch_id=468: 100%|██████████| 469/469 [00:19<00:00, 24.27it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 1.8792, Accuracy: 2924/10000 (29%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}